services:

  # --------------------
  # Kafka & Confluent
  # --------------------
  zookeeper:
    container_name: zookeeper
    image: confluentinc/cp-zookeeper:7.5.0
    env_file:
      - envs/.env.zookeeper
    volumes:
      - ./volumes/.zookeeper/zk_data:/var/lib/zookeeper/data
      - ./volumes/.zookeeper/zk_logs:/var/lib/zookeeper/log
    ports:
      - "22182:2181"
    networks:
      - matchstream

  kafka_broker:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9093:9092"
      - "29093:29092"
    env_file:
      - envs/.env.kafka
    environment:
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka_broker:9092,PLAINTEXT_HOST://localhost:29093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
    volumes:
      - ./volumes/.kafka_data:/var/lib/kafka/data
    networks:
      - matchstream

  control_center:
    image: confluentinc/cp-enterprise-control-center:7.5.0
    container_name: control_center
    depends_on:
      - kafka_broker
      - zookeeper
    env_file:
      - envs/.env.control_center
    ports:
      - "9022:9021"
    networks:
      - matchstream

  # --------------------
  # Postgres, Hive Metastore 
  # --------------------
  pg_hive_db:
    image: 'postgres:17'
    hostname: postgres
    container_name: pg_hive_db
    ports:
      - '5434:5432'
    env_file:
      - envs/.env.pg_hive_db
    volumes:
      - ./volumes/.pg_hive_db:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h localhost -p 5432 -U admin -d metastore_db"]
      interval: 5s
      timeout: 5s
      retries: 20
    networks:
      - matchstream

  hive-metastore:
    image: 'naushadh/hive-metastore'
    container_name: hive-metastore
    hostname: hive-metastore
    ports:
      - '9083:9083'
    env_file:
      - envs/.env.pg_hive_db
    depends_on:
      pg_hive_db:
        condition: service_healthy
      minio:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "bash -c 'echo > /dev/tcp/localhost/9083'"]
      interval: 5s
      timeout: 5s
      retries: 20
    networks:
      - matchstream

  # --------------------
  # DB section
  # --------------------
  pg_writer:
    image: postgres:17
    container_name: pg_writer
    env_file:
      - envs/.env.pg_writer
    ports:
      - "5435:5432"
    volumes:
      - ./volumes/pg_prod/writer:/var/lib/postgresql/data
      - ./docker/scripts/pg_prod/init_writer.sh:/docker-entrypoint-initdb.d/init_writer.sh
    command: >
      postgres
      -c wal_level=logical
      -c max_wal_senders=10
      -c max_replication_slots=10
      -c hot_standby=on
    networks:
      - matchstream

  pg_replica:
    image: postgres:17
    container_name: pg_replica
    env_file:
      - envs/.env.pg_replica
    ports:
      - "5436:5432"
    volumes:
      - ./volumes/pg_prod/replica:/var/lib/postgresql/data_replica
      - ./docker/scripts/pg_prod/init_replica.sh:/docker-entrypoint-initdb.d/init_replica.sh
    networks:
      - matchstream
    depends_on:
      - pg_writer
      
  # --------------------
  # Spark Streaming
  # --------------------
  spark_streaming:
    build:
      context: .
      dockerfile: ./docker/docker_files/Dockerfile.spark_streaming
    container_name: spark_streaming
    working_dir: /opt/streaming/jobs
    tmpfs:
      - /tmp:size=4G,exec
    ports:
      - "7077:7077" 
      - "4041:4040"    # Spark UI (different port)
    mem_limit: 8g
    mem_reservation: 6g
    volumes:
      - ./volumes/.spark_warehouse:/spark-warehouse
      - ./configs/spark_iceberg/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./configs/spark_iceberg/hive-site.xml:/opt/spark/conf/hive-site.xml
      - ./configs:/opt/streaming/configs
      - ./jobs:/opt/streaming/jobs
    env_file:
      - envs/.env.spark_streaming
    depends_on:
      - hive-metastore
      - kafka_broker
      - minio
    command: tail -f /dev/null
    networks:
      - matchstream
    restart: unless-stopped

  # --------------------
  # Airflow
  # --------------------
  pg_airflow_db:
    image: 'postgres:17'
    container_name: pg_airflow_db
    ports:
      - '5437:5432'
    env_file:
      - envs/.env.pg_airflow_db
    volumes:
      - ./volumes/.pg_airflow_db:/var/lib/postgresql/data
      - ./docker/scripts/airflow/airflow_db_init.sh:/docker-entrypoint-initdb.d/init-db.sh
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d airflow || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 15
    networks:
      - matchstream

  airflow-scheduler:
    image: apache/airflow:2.10.1-python3.11
    depends_on:
      pg_airflow_db:
        condition: service_healthy
    container_name: airflow_scheduler
    env_file:
      - envs/.env.airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./docker/scripts/airflow/airflow_scheduler_entrypoint.sh:/opt/airflow/airflow_scheduler_entrypoint.sh
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - matchstream
    healthcheck:
      test: ["CMD", "airflow", "jobs", "check", "--job-type", "SchedulerJob"]
      interval: 15s
      timeout: 5s
      retries: 5
    entrypoint: ["/opt/airflow/airflow_scheduler_entrypoint.sh"]

  airflow-webserver:
    image: apache/airflow:2.10.1-python3.11
    container_name: airflow_webserver
    depends_on:
      pg_airflow_db:
        condition: service_healthy
      airflow-scheduler:
        condition: service_healthy
    ports:
      - "8085:8080"
    env_file:
      - envs/.env.airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./docker/scripts/airflow/airflow_webserver_entrypoint.sh:/opt/airflow/airflow_webserver_entrypoint.sh
      - /var/run/docker.sock:/var/run/docker.sock
    entrypoint: ["/opt/airflow/airflow_webserver_entrypoint.sh"]
    networks:
      - matchstream

  # --------------------
  # MinIO
  # --------------------
  minio:
    image: minio/minio:RELEASE.2024-05-10T01-41-38Z
    hostname: minio
    container_name: minio
    ports:
      - '9000:9000'
      - '9001:9001'
    volumes:
      - ./volumes/.minio:/data
    env_file:
      - envs/.env.minio
    command: server --console-address ":9001" /data
    networks:
      - matchstream

  minio_init:
    image: minio/mc
    depends_on:
      - minio
    env_file:
      - envs/.env.minio
    entrypoint: ["/bin/sh", "/scripts/init.sh"]
    volumes:
    - ./docker/scripts/minio/minio_buckets_init.sh:/scripts/init.sh:ro
    networks:
      - matchstream

  trino:
    image: trinodb/trino:435
    container_name: trino
    hostname: trino
    ports:
      - "8082:8080"
    volumes:
      - ./configs/trino:/etc/trino
    env_file:
      - envs/.env.trino
    depends_on:
      - hive-metastore
      - minio
    networks:
      - matchstream

  # User generator python container
  python_user_generator:
    build:
      context: ./matchstream_app
      dockerfile: user_generator/Dockerfile.user_generator
    container_name: python_user_generator
    working_dir: /app/user_generator
    environment:
      PYTHONPATH: /app/user_generator
    networks:
      - matchstream
    volumes:
      - ./matchstream_app:/app
    tty: true
    stdin_open: true

  # User generator python container
  python_user_ingestor_to_writer:
    build:
      context: ./matchstream_app
      dockerfile: user_ingestor_to_writer/Dockerfile.user_ingestor_to_writer
    container_name: python_user_ingestor_to_writer
    working_dir: /app/user_ingestor_to_writer
    environment:
      PYTHONPATH: /app/user_ingestor_to_writer
    env_file:
      - envs/.env.python_user_ingestor_to_writer
    volumes:
      - ./matchstream_app:/app
    tty: true
    stdin_open: true
    depends_on:
      - pg_writer
    networks:
      - matchstream

  # --------------------
  # Spark Thrift & DBT
  # --------------------
  spark_thriftserver:
    build:
      context: .
      dockerfile: ./docker/docker_files/Dockerfile.spark_thriftserver
    container_name: spark-thriftserver
    hostname: spark-thriftserver
    ports:
      - "10000:10000"
      - "10001:10001"  # Thrift server for dbt JDBC
      - "4043:4040"    # Spark UI
    volumes:
      - ./configs/spark_thrift/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./configs/spark_iceberg/hive-site.xml:/opt/spark/conf/hive-site.xml
    env_file:
      - ./envs/.env.spark_thriftserver
    command: ["/start_thrift.sh"]
    depends_on:
      hive-metastore:
        condition: service_healthy
    networks:
      - matchstream

  dbt_spark:
    build:
      context: .
      dockerfile: ./dbt_matchstream/Dockerfile.dbt_matchstream
    container_name: dbt_spark
    volumes:
      - ./dbt_matchstream:/workspace
    environment:
      DBT_PROFILES_DIR: /workspace
    networks:
      - matchstream
    depends_on:
      - spark_thriftserver
    tty: true
    stdin_open: true

  # --------------------
  # MatchStream APP Frontend and Backend
  # --------------------
  matchstream_backend:
    build:
      context: ./matchstream_app
      dockerfile: matchstream_backend/Dockerfile.matchstream_backend
    container_name: matchstream_backend
    working_dir: /app
    environment:
      PYTHONPATH: /app
    env_file:
      - envs/.env.matchstream_backend
    ports:
      - "8010:8010"
    networks:
      - matchstream
    volumes:
      - ./matchstream_app:/app
    tty: true
    stdin_open: true

  matchstream_frontend:
    build:
      context: ./matchstream_app
      dockerfile: matchstream_frontend/Dockerfile.matchstream_frontend
    container_name: matchstream_frontend
    working_dir: /app/matchstream_frontend
    environment:
      PYTHONPATH: /app/matchstream_frontend
    ports:
      - "8501:8501"
    networks:
      - matchstream
    volumes:
      - ./matchstream_app:/app
    tty: true
    stdin_open: true

networks:
  matchstream:
    driver: bridge